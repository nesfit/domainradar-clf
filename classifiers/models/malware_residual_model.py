import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


class Net(nn.Module):
    def __init__(self, feature_size):
        super(Net, self).__init__()

        # Complex fully connected layers with more configurations
        self.fc1 = nn.Linear(feature_size, 2048)
        self.fc2 = nn.Linear(2048, 1024)
        self.fc3 = nn.Linear(1024, 512)
        self.fc4 = nn.Linear(512, 256)
        self.fc5 = nn.Linear(256, 128)
        self.fc6 = nn.Linear(128, 64)
        self.fc7 = nn.Linear(64, 1)

        # Batch Normalization layers for each FC layer
        self.bn1 = nn.BatchNorm1d(2048)
        self.bn2 = nn.BatchNorm1d(1024)
        self.bn3 = nn.BatchNorm1d(512)
        self.bn4 = nn.BatchNorm1d(256)
        self.bn5 = nn.BatchNorm1d(128)

        # Adaptive dropout rates

        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.15)
        self.dropout3 = nn.Dropout(0.1)

        # More complex residual connections
        self.shortcut1 = nn.Sequential(
            nn.Linear(feature_size, 1024), nn.BatchNorm1d(1024)
        )
        self.shortcut2 = nn.Sequential(nn.Linear(1024, 256), nn.BatchNorm1d(256))

        # Adaptive pooling
        self.pool = nn.AdaptiveAvgPool1d(1)

    def forward(self, x):
        identity = x

        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout1(x)
        x += self.shortcut1(identity)  # Residual connection

        identity = x
        x = F.relu(self.bn3(self.fc3(x)))
        x = F.relu(self.bn4(self.fc4(x)))
        x = self.dropout2(x)
        x += self.shortcut2(identity)  # Residual connection

        x = F.relu(self.bn5(self.fc5(x)))
        x = self.dropout3(x)
        x = F.relu(self.fc6(x))
        x = self.fc7(x)

        x = x.unsqueeze(-1)  # Necessary for AdaptiveAvgPool1d
        x = self.pool(x)
        return x.squeeze(-1)

    def configure_optimizers(self, lr):
        # Parameter groups for differential learning rates
        base_params = [self.fc1, self.fc2, self.fc3]
        fine_tune_params = [
            self.fc4,
            self.fc5,
            self.fc6,
            self.fc7,
            self.shortcut1,
            self.shortcut2,
        ]

        optimizer = optim.Adam(
            [
                {"params": base_params, "lr": lr},
                {"params": fine_tune_params, "lr": 1e-4},
            ],
            weight_decay=0.01,
        )

        return optimizer
