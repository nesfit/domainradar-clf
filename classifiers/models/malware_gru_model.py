import torch
import torch.nn as nn
import torch.nn.functional as F


import torch
import torch.nn as nn
import torch.nn.functional as F


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


class Net(nn.Module):
    def __init__(self, input_channels: int):
        super(Net, self).__init__()

        # GRU layer to capture long-term dependencies
        self.gru = nn.GRU(input_channels, 512, batch_first=True)

        # Fully connected layers
        self.fc1 = nn.Linear(512, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 128)
        self.fc4 = nn.Linear(128, 1)  # Output layer for binary classification

        # Dropout and Batch Normalization
        self.dropout = nn.Dropout(0.1)
        self.batchnorm1 = nn.BatchNorm1d(64)
        self.batchnorm2 = nn.BatchNorm1d(64)
        self.batchnorm3 = nn.BatchNorm1d(1024)

    def forward(self, x):
        # x shape expected: (batch_size, input_channels, sequence_length)
        # Convolutional layers with Batch Norm and ReLU activations

        x, _ = self.gru(x)

        x = self.dropout(F.relu(self.batchnorm3(self.fc1(x))))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))

        return self.fc4(x)

    def configure_optimizers(self, lr=1e-3):
        # Define an optimizer with different learning rates
        optimizer = optim.Adam(
            [
                {
                    "params": list(self.gru.parameters()),
                    "lr": lr * 0.5,
                },  # Correct way to pass parameters
                {
                    "params": list(self.fc1.parameters())
                    + list(self.fc2.parameters())
                    + list(self.fc3.parameters())
                    + list(self.fc4.parameters()),
                    "lr": lr,
                },
            ],
            weight_decay=0.01,
        )

        return optimizer

    def initialize_weights(self):
        # Initialize weights using a uniform distribution
        for m in self.modules():
            if isinstance(m, (nn.Linear, nn.GRU)):
                nn.init.kaiming_uniform_(m.weight.data)

    def summary(self):
        # Print a summary of the model
        for name, param in self.named_parameters():
            print(f"{name} | Size: {param.size()} | Values : {param[:2]} \n")


# Example of usage:
